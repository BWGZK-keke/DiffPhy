<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Think Before You Diffuse: LLMs-guided Physics-Aware Video Generation</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Inter', sans-serif;
      line-height: 1.6;
      max-width: 900px;
      margin: 40px auto;
      padding: 0 24px;
      background-color: #f4f6f8;
      color: #2c3e50;
    }

    h1 {
      text-align: center;
      font-size: 2.5rem;
      font-weight: 700;
      color: #1a1a1a;
      margin-bottom: 10px;
    }

    .authors,
    .affiliation {
      text-align: center;
      font-size: 1.1rem;
      color: #444;
    }

    .affiliation {
      font-size: 1rem;
      color: #666;
      margin-bottom: 30px;
    }

    .links {
      margin-top: 20px;
      text-align: center;
    }

    .links a {
      display: inline-block;
      margin: 10px 10px 0 0;
      padding: 10px 16px;
      font-size: 0.95rem;
      font-weight: 600;
      text-decoration: none;
      color: #ffffff;
      background-color: #007acc;
      border-radius: 8px;
      transition: background-color 0.3s ease;
    }

    .links a:hover {
      background-color: #005fa3;
    }

    .links a[disabled] {
      background-color: #999999;
      pointer-events: none;
    }

    .gif-row {
      display: flex;
      justify-content: space-between;
      align-items: center;
      gap: 20px;
      margin: 40px 0;
      flex-wrap: wrap;
    }

    .gif-row img {
      width: 100%;
      max-width: 280px;
      border-radius: 12px;
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
    }

    h2 {
      text-align: center;
      font-size: 1.6rem;
      margin-top: 40px;
      color: #2c3e50;
    }

    p {
      font-size: 1.05rem;
      color: #333;
    }

    video {
      max-width: 100%;
      margin-top: 20px;
      border-radius: 12px;
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
    }

    .footer {
      margin-top: 60px;
      font-size: 0.9em;
      color: #777;
      text-align: center;
    }
    .description-box {
    background-color: #ffffff;
    padding: 24px;
    border-radius: 12px;
    box-shadow: 0 4px 14px rgba(0, 0, 0, 0.06);
    margin-top: 40px;
  }

  .description-box h2 {
    font-size: 1.6rem;
    margin-bottom: 16px;
    border-bottom: 2px solid #ddd;
    padding-bottom: 8px;
  }

  .description-box p {
    font-size: 1.05rem;
    margin-bottom: 16px;
    line-height: 1.7;
  }

  .description-box ul {
    padding-left: 1.2em;
    font-size: 1.05rem;
    margin-bottom: 12px;
  }

  .description-box li {
    margin-bottom: 10px;
  }

  .description-box li::marker {
    color: #007acc;
  }

  .description-box ul ul li::marker {
    color: #888;
  }
    .section-box {
    background-color: #ffffff;
    padding: 24px;
    border-radius: 12px;
    box-shadow: 0 4px 14px rgba(0, 0, 0, 0.06);
    margin-top: 40px;
  }

  .section-box h2 {
    font-size: 1.6rem;
    margin-bottom: 16px;
    border-bottom: 2px solid #ddd;
    padding-bottom: 8px;
  }

.pipeline-img {
  max-width: 100%;
  height: auto;
  image-rendering: auto; /* or use crisp-edges for pixel art */
  border-radius: 12px;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
}

  .pipeline-img {
    width: 100%;
    max-width: 100%;
    border-radius: 12px;
    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
    margin-top: 16px;
  }

  .bibtex-box {
    background-color: #f9f9f9;
    font-family: 'Courier New', Courier, monospace;
    padding: 16px;
    border-radius: 8px;
    overflow-x: auto;
    margin-top: 16px;
    white-space: pre-wrap;
    font-size: 0.95rem;
    border-left: 4px solid #007acc;
  }
  </style>
</head>
<body>

  <h1>Think Before You Diffuse: LLMs-guided Physics-Aware Video Generation</h1>

  <div class="authors">
    Ke Zhang, Cihan Xiao, Jiacong Xu, Yiqun Mei, Vishal M. Patel
  </div>
  <div class="affiliation">
    Johns Hopkins University
  </div>

  <div class="links">
    <a href="https://arxiv.org/pdf/2505.21653" target="_blank">üìÑ Paper (arXiv)</a>
    <a href="#" onclick="alert('Code coming soon!')" style="background-color: #999999;">üíª Code (Coming Soon)</a>
    <a href="#" onclick="alert('Model coming soon!')" style="background-color: #999999;">üß† Model (Coming Soon)</a>
    <a href="#" onclick="alert('Dataset coming soon!')" style="background-color: #999999;">üìÅ Dataset (Coming Soon)</a>
  </div>

  <div class="gif-row">
    <img src="tutu.gif" alt="Demo GIF 1">
    <img src="sports.gif" alt="Demo GIF 2">
    <img src="sink.gif" alt="Demo GIF 3">
  </div>

  <h2>Demo Video</h2>
  <video controls>
    <source src="Demo-small.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>

<div class="description-box">
  <h2>Abstract</h2>
Recent video diffusion models have demonstrated their great capability in generating visually-pleasing results, while synthesizing the correct physical effects in generated videos remains challenging. The complexity of real-world motions, interactions, and dynamics introduce great difficulties when learning physics from data. In this work, we propose DiffPhy, a generic framework that enables physically-correct and photo-realistic video generation by fine-tuning a pre-trained video diffusion model. Our method leverages large language models (LLMs) to explicitly reason a comprehensive physical context from the text prompt and use it to guide the generation. To incorporate physical context into the diffusion model, we leverage a Multimodal large language model (MLLM) as a supervisory signal and introduce a set of novel training objectives that jointly enforce physical correctness and semantic consistency with the input text. We also establish a high-quality physical video dataset containing diverse phyiscal-related actions and events to facilitate effective finetuning. Extensive experiments on public benchmarks, demonstrate that DiffPhy is able to produce state-of-the-art results across diverse physical scenarios. Our model and data will be released after the review process.
</div>

<!-- Method Pipeline -->
<div class="section-box">
  <h2>Method Pipeline</h2>
  <div style="text-align: center; overflow-x: auto;">
    <img 
      src="pipeline.png" 
      alt="Method Pipeline Diagram" 
      class="pipeline-img" 
      style="max-width: 100%; height: auto; image-rendering: -webkit-optimize-contrast; image-rendering: crisp-edges;"
    >
  </div>
  <p style="margin-top: 20px; font-size: 1.05rem; line-height: 1.7; color: #333;">
    <strong>An overview of DiffPhy.</strong> Our method contains five steps: <strong>1.</strong>strong> Given a user prompt, we leverage a pretrained LLM to reason physical properties from the text input. <strong>2.</strong>strong> We then (a) enhance the user prompt with physical context and (b) produce a list of relevant physical phenomena associated with the described event. <strong> 3.</strong> We use the enhanced prompt to guide video generation. The phenomena list is used to penalize outputs with implausible physics. <strong> 4. </strong>strong> A set of novel training objectives are proposed to jointly enforce physical correctness and semantic consistency.</p>
</div>
<!-- <div class="section-box">
  <h2>Leaderboard on PhyGenBench</h2>
  <p style="font-size: 1.05rem; margin-bottom: 16px;">
    Quantitative comparison across four physical domains. Our method <strong>DiffPhy</strong> achieves the highest average performance, showcasing its superiority in generating physically-plausible videos. The compared results are borrowed directly from the PhyGenBench
  </p>
  <div style="overflow-x: auto;">
    <table style="width: 100%; border-collapse: collapse; font-size: 1.05rem; text-align: center;">
      <thead style="background-color: #007acc; color: #fff;">
        <tr>
          <th style="padding: 10px; border-radius: 8px 0 0 0;">Model</th>
          <th style="padding: 10px;">Mechanics ‚Üë</th>
          <th style="padding: 10px;">Optics ‚Üë</th>
          <th style="padding: 10px;">Thermal ‚Üë</th>
          <th style="padding: 10px;">Material ‚Üë</th>
          <th style="padding: 10px; border-radius: 0 8px 0 0;">Average</th>
        </tr>
      </thead>
      <tbody style="background-color: #fff;">
        <tr><td style="padding: 10px;">CogVideoX-2B</td><td>0.38</td><td>0.43</td><td>0.34</td><td>0.39</td><td>0.37</td></tr>
        <tr><td style="padding: 10px;">CogVideoX-5B</td><td>0.43</td><td>0.55</td><td>0.40</td><td>0.42</td><td>0.45</td></tr>
        <tr><td style="padding: 10px;">Open-Sora V1.2</td><td>0.43</td><td>0.50</td><td>0.44</td><td>0.37</td><td>0.44</td></tr>
        <tr><td style="padding: 10px;">Lavie</td><td>0.40</td><td>0.44</td><td>0.38</td><td>0.32</td><td>0.36</td></tr>
        <tr><td style="padding: 10px;">Vchitect 2.0</td><td>0.41</td><td>0.56</td><td>0.44</td><td>0.37</td><td>0.45</td></tr>
        <tr><td style="padding: 10px;">Pika</td><td>0.35</td><td>0.56</td><td>0.43</td><td>0.39</td><td>0.44</td></tr>
        <tr><td style="padding: 10px;">Kling</td><td>0.45</td><td>0.58</td><td>0.50</td><td>0.40</td><td>0.49</td></tr>
        <tr><td style="padding: 10px;">Wan</td><td>0.36</td><td>0.53</td><td>0.36</td><td>0.33</td><td>0.40</td></tr>
        <tr style="background-color: #e8f4ff; font-weight: 600;">
          <td style="padding: 10px;">DiffPhy (ours)</td><td>0.53</td><td>0.59</td><td>0.58</td><td>0.46</td><td>0.54</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>
 -->

<!-- BibTeX Citation -->
<div class="section-box">
  <h2>BibTeX Citation</h2>
  <div class="bibtex-box">
@misc{zhang2025think,<br>
&nbsp;&nbsp;title={Think Before You Diffuse: LLMs-Guided Physics-Aware Video Generation},<br>
&nbsp;&nbsp;author={Ke Zhang, Cihan Xiao, Jiacong Xu, Yiqun Mei, Vishal M. Patel},<br>
&nbsp;&nbsp;year={2025},<br>
&nbsp;&nbsp;eprint={2505.21653},<br>
&nbsp;&nbsp;archivePrefix={arXiv},<br>
&nbsp;&nbsp;primaryClass={cs.CV}<br>
}
  </div>
</div>

</body>
</html>
