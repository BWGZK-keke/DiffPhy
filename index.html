<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Think Before You Diffuse: LLMs-guided Physics-Aware Video Generation</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      line-height: 1.6;
      max-width: 800px;
      margin: 40px auto;
      padding: 0 20px;
      background-color: #f9f9f9;
      color: #333;
    }
    h1 {
      color: #2c3e50;
    }
    h2 {
      margin-top: 30px;
      color: #34495e;
    }
    .meta, .keywords {
      font-size: 0.95em;
      color: #555;
    }
    .authors {
      margin: 10px 0;
    }
    video {
      max-width: 100%;
      margin-top: 20px;
    }
    .footer {
      margin-top: 40px;
      font-size: 0.9em;
      color: #888;
    }
  </style>
</head>
<body>

  <h1>Think Before You Diffuse: LLMs-guided Physics-Aware Video Generation</h1>

  <div class="authors">
    <strong>Ke Zhang</strong>, Cihan Xiao, Jiacong Xu, Yiqun Mei, Vishal M. Patel
  </div>

  <h2>Abstract</h2>
  <p>
    Recent video diffusion models have demonstrated their great capability in generating visually-pleasing results, while synthesizing the correct physical effects in generated videos remains challenging. The complexity of real-world motions, interactions, and dynamics introduce great difficulties when learning physics from data. In this work, we propose <strong>DiffPhy</strong>, a generic framework that enables physically-correct and photo-realistic video generation by fine-tuning a pre-trained video diffusion model.
  </p>
  <p>
    Our method leverages large language models (LLMs) to explicitly reason a comprehensive physical context from the text prompt and use it to guide the generation. To incorporate physical context into the diffusion model, we leverage a Multimodal large language model (MLLM) as a supervisory signal and introduce a set of novel training objectives that jointly enforce physical correctness and semantic consistency with the input text.
  </p>
  <p>
    We also establish a high-quality physical video dataset containing diverse physical-related actions and events to facilitate effective finetuning. Extensive experiments on public benchmarks demonstrate that DiffPhy is able to produce state-of-the-art results across diverse physical scenarios. Our model and data will be released after the review process.
  </p>

  <h2>Project Video</h2>
  <!-- Replace 'video.mp4' with your actual video filename or YouTube iframe if needed -->
  <video controls>
    <source src="Demo-small.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>

  <div class="footer">
    © 2025 Ke Zhang et al. — Contact: kezhang [at] jhu.edu
  </div>

</body>
</html>

